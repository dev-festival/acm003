---
title: "AAP ACM Coverage to Standard"
subtitle: "Implementation of Standards"
author: "Mike"
date: today
format:
  revealjs:
    theme: [default]
    slide-number: true
    embed-resources: true
    preview-links: auto
    navigation-mode: linear
    controls: true
    progress: true
    history: true
    center: true
    transition: slide
    background-transition: fade
    width: 1280
    height: 1080
    incremental: false
    smaller: true
    scrollable: true
    df-print: kable # or 'paged' or 'default'
execute:
  echo: false
  warning: false
---

```{python}
#| echo: false
#| label: import-libraries

import pandas as pd
import numpy as np
import os
import pyodbc
import re
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
import warnings
import time       # for timing longer cells (running the sql query)
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime,timedelta
from IPython.display import Markdown
```

```{python}
#| echo: false
#| label: ODBC-credentials

# Automatically search for .env in parent directories
load_dotenv(find_dotenv())

# Load from environment variables
DSN = os.getenv('MAXIMO_DSN')
USER = os.getenv('MAXIMO_USER')
PASSWORD = os.getenv('MAXIMO_PASS')

#Confirm Credentials
#print(f"DSN: {DSN}")
#print(f"User: {USER}")
```

```{python}
#| echo: false
#| label: query-function

# Suppress the pandas warning
warnings.filterwarnings('ignore', category=UserWarning)

def run_query_from_file(sql_path: str) -> pd.DataFrame:
    conn = pyodbc.connect(f"DSN={DSN};UID={USER};PWD={PASSWORD}")
    query = open(sql_path).read()
    df = pd.read_sql(query, conn)
    conn.close()
    return df
```

# Phase 0 - Introduction {background-color="#1e3a8a"}

## Project Overview {.smaller}

Demonstrate implementation of standards from:

::: {.columns}
::: {.column width="60%"}
- [MESD-ACM-0001](https://globalhonda.sharepoint.com/:w:/r/sites/gna00880-08-Reliability/Shared%20Documents/08-%20Reliability/04-ACM/00_Overall%20Documents/05_Frameworks/000%20-%20Standard%20for%20Tracking%20ACM%20Technology%20Application.docx?d=w9b9e0e5d7f2c4fdbb8f5ee2ce4919adb&csf=1&web=1&e=uMqNtS) - Standard for Tracking (Has Monitoring)
- [Draft 001](https://globalhonda.sharepoint.com/:w:/r/sites/gna00880-08-Reliability/Shared%20Documents/08-%20Reliability/04-ACM/00_Overall%20Documents/05_Frameworks/001%20-%20Asset%20Condition%20Monitoring%20Technology%20Application%20Specification.docx?d=wb984d67b7b004337aa6b469a06190d85&csf=1&web=1&e=JHKuoG) - Technology Application (Needs Monitoring)


**Goal:** Compare Coverage Needs vs Coverage Actuals
:::

:::{.column width ="40%"}
```{mermaid}
%%| label: mermaid-coverage-report

%%| fig-width: 8
flowchart LR
    A[Has Monitoring] --> C[Coverage Report]
    B[Needs Monitoring] --> C
    
    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#fff4e1
```

::: {.notes}
Two-part analysis: what we NEED to monitor vs what we ARE monitoring
:::

:::

:::

{background-color="#6aca8eff"}

## Technology Definitions 

Technologies are defined in MESD-ACM-001 as: 

```{python}
#| echo: false
#| label: technology-defitinitions
#| tbl-cap: "Technology Codes and Definitions"

tech_codes = pd.read_csv('data/st_tbl/techs_def.csv',
                            dtype={'domain': 'category',
                                    'tech_code': 'category'}
                        )
tech_cols = tech_codes['tech_code'].tolist()

tech_codes
```

## Data Sources Overview

1. **Has Monitoring:** Uses the MESD-ACM-001 Standard for how to pull that information
    - Disconnected Database: `Maximo` ODBC connection to pull Routes & Meters for "Has Monitoring"
    - Connected Database: `API` python connection
    - Some databases are *Connectable* but do not have active connections yet
2. **Needs Monitoring:** Uses the MESD-ACM-002 Standard for how to define what needs monitoring
    - Asset Classes are pulled from `Maximo` using ODBC connection 
    - Technologies are mapped to components from `comp_xref_tech.csv`
    - Components are mapped to asset classes from `class_xref_comp.csv`


# Phase I - Needs Monitoring {background-color="#15803d"}

## Definition {.smaller}

 What assets *should* have monitoring based on their class/components? 

```{mermaid}
%%| label: mermaid-monitoring-map

flowchart LR
COMPMAP[Component Map to Technology]
CLASSMAP[Asset Class Mapped to Component]
PYTHMERGE[Python Merge]
OUTPUT[Needs Monitoring Map]

COMPMAP --> PYTHMERGE
CLASSMAP --> PYTHMERGE
PYTHMERGE --> OUTPUT
```


## Needs Monitoring: Definition {.smaller}

"Needs Monitoring" is determined by the asset's **class** and **components**.

### Asset Class to Component Mapping {.smaller}

The `class_xref_comp.csv` table cross-references:

- **Asset Classes** (pulled from Maximo) 
- **Components** that each class *should* have, list developed with RE's (inside MESD-ACM-002) [doc ref??]

This establishes the expected component inventory for each asset type.

**Example**: A "Pump" class should have components like:

- Motor
- Bearings
- Coupling
- Seal assembly

## Mapping Logic

1. **Start with Asset Class** (from Maximo)
2. **Identify Required Components** (via `class_xref_comp.csv`)
3. **Determine Required Technologies** (via `comp_xref_tech.csv`)
4. **Result**: Boolean flags for which technologies this asset *needs*

## Mapping Tables {.smaller}

::: panel-tabset
### Class → Component Map

::: {style="max-height: 500px; overflow-y: auto; font-size: 0.55em;"}
```{python}
#| echo: false
#| label: class-comp-read-in


class_comp = pd.read_csv('data/st_tbl/class_xref_comp.csv',index_col = 0)
class_comp
```
:::


### Component → Tech Map

P = Primary Technology

S = Secondary Technology

::: {style="max-height: 500px; overflow-y: auto; font-size: 0.55em;"}
```{python}
#| echo: false
#| label: comp-tech-read-in


comp_tech = pd.read_csv('data/st_tbl/comp_xref_tech.csv')
comp_tech.style.set_table_attributes('style="max-height: 400px; overflow-y: scroll; display: block;"')
```
:::

:::

## Statistics So Far

```{python}
#| echo: false
#| label: stats-so-far

print(f"Asset classes: {len(class_comp)}")
print(f"Components: {len(comp_tech)}")
print(f"Technologies tracked: {tech_cols}")
```


```{python}
#| echo: false
#| label: needs-monitoring-function

def get_asset_tech_needs(component_list, comp_tech_df, tech_cols, include_secondary=True):
    """
    Determine which technologies an asset NEEDS based on its components.
    
    Parameters:
    -----------
    component_list : list
        List of component types for the asset
    comp_tech_df : DataFrame
        Component to technology mapping (comp_xref_tech.csv)
    tech_cols : list
        List of technology codes to check
    include_secondary : bool
        If True, include both P and S ratings. If False, only P ratings.
    
    Returns:
    --------
    dict : {'IR': 'P', 'VI': 'S', 'LU': 'N', ...}
        Priority: P (Primary) > S (Secondary) > N (Not needed)
    """
    # Initialize all technologies as 'N' (not needed)
    asset_needs = {tech: 'N' for tech in tech_cols}
    
    if not component_list or len(component_list) == 0:
        return asset_needs
    
    # For each component, get its technology requirements
    for component in component_list:
        comp_row = comp_tech_df[comp_tech_df['Component Type'] == component]
        
        if comp_row.empty:
            continue
        
        # Check each technology
        for tech in tech_cols:
            if tech not in comp_row.columns:
                continue
                
            component_rating = comp_row[tech].iloc[0]
            current_rating = asset_needs[tech]
            
            # Update if this component has higher priority
            # Priority: P (Primary) > S (Secondary) > N
            if component_rating == 'P':
                asset_needs[tech] = 'P'
            elif component_rating == 'S' and current_rating == 'N':
                asset_needs[tech] = 'S'
    
    # If include_secondary is False, convert S to N (keep only Primary)
    if not include_secondary:
        asset_needs = {k: ('N' if v == 'S' else v) for k, v in asset_needs.items()}
    
    return asset_needs
```

```{python}
#| echo: false
#| label: build-needs-coverage

# Build needs coverage for all asset classes
needs_records = []

for asset_class in class_comp.index:
    # Get components for this asset class (where 'x' is marked)
    components = []
    for col in class_comp.columns:
        if col == 'Asset_Count':
            continue
        if class_comp.loc[asset_class, col] == 'x':
            components.append(col)
    
    # Get technology needs for this asset class
    tech_needs = get_asset_tech_needs(components, comp_tech, tech_cols, include_secondary=True)
    
    # Build record
    record = {'ASSETCLASS': asset_class}
    for tech in tech_cols:
        record[f'NEEDS_{tech}'] = tech_needs.get(tech, 'N')
    
    needs_records.append(record)

# Create DataFrame
needs_coverage = pd.DataFrame(needs_records)

# Summary statistics
print(f"\nTotal asset classes: {len(needs_coverage)}")
print("\nTechnology needs distribution:")
for tech in tech_cols:
    col = f'NEEDS_{tech}'
    if col in needs_coverage.columns:
        p_count = (needs_coverage[col] == 'P').sum()
        s_count = (needs_coverage[col] == 'S').sum()
        print(f"  {tech}: {p_count} Primary, {s_count} Secondary")

needs_coverage.info()

```

```{python}
#| echo: false
#| label: output-needs-coverage

# Save final coverage report
needs_coverage.to_pickle('data/needs_coverage.pkl')
needs_coverage.to_csv('output/intermediate/needs_coverage.csv', index=False)

print(f"\nSaved needs coverage report:")
print(f"  - data/needs_coverage.pkl ({len(needs_coverage):,} asset classes)")
print(f"  - output/needs_coverage.csv")
```

## Visualizing Technology Needs

::: panel-tabset

### Class by Component

```{python}
#| echo: false
#| label: fig-top-classes
#| fig-cap: "Top 15 Asset Classes by Number of Monitorable Components"


# Count components per asset class (number of 'x' marks)
class_comp['component_count'] = (class_comp.drop('Asset_Count', axis=1) == 'x').sum(axis=1)

# Get top 15
top_classes = class_comp.nlargest(15, 'component_count')[['component_count']].sort_values('component_count')

# Plot
fig, ax = plt.subplots(figsize=(10, 8))
top_classes.plot(kind='barh', ax=ax, color='steelblue', legend=False)
ax.set_xlabel('Number of Monitorable Components')
ax.set_ylabel('Asset Class')
ax.set_title('Top 15 Asset Classes by Component Count')
ax.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
```

### Technology Heatmap
```{python}
#| echo: false
#| label: fig-component-tech-heatmap
#| fig-cap: "Component Technology Requirements Heatmap"

# Prepare data for heatmap
# Convert P/S/- to numeric: P=2, S=1, -=0
comp_tech_numeric = comp_tech.copy()
comp_tech_numeric = comp_tech_numeric.set_index('Component Type')

# Replace ratings with numeric values
rating_map = {'P': 2, 'S': 1, '-': 0, np.nan: 0}
for col in comp_tech_numeric.columns:
    comp_tech_numeric[col] = comp_tech_numeric[col].map(rating_map).fillna(0)

# Count technologies per component
comp_tech_numeric['tech_count'] = (comp_tech_numeric > 0).sum(axis=1)

# Create heatmap
fig, ax = plt.subplots(figsize=(10, 12))
sns.heatmap(
    comp_tech_numeric.drop('tech_count', axis=1),
    cmap=['#f0f0f0', '#ffeb99', '#4CAF50'],  # 0=gray, 1=yellow (S), 2=green (P)
    cbar_kws={'label': 'Priority (0=None, 1=Secondary, 2=Primary)'},
    linewidths=0.5,
    linecolor='white',
    ax=ax,
    vmin=0,
    vmax=2
)
ax.set_title('Component Technology Requirements\n(Green=Primary, Yellow=Secondary, Gray=None)')
ax.set_xlabel('Technology Code')
ax.set_ylabel('Component Type')
plt.tight_layout()
plt.show()

# Also show top components by technology count
print("\nTop 10 Components by Number of Technologies:")
print(comp_tech_numeric.nlargest(10, 'tech_count')[['tech_count']])
```

### Technology Distribution
```{python}
#| echo: false  
#| label: fig-tech-distribution
#| fig-cap: "Technology Needs Distribution Across Asset Classes"

# Count P and S ratings per technology
tech_summary = []
for tech in tech_cols:
    col = f'NEEDS_{tech}'
    if col in needs_coverage.columns:
        p_count = (needs_coverage[col] == 'P').sum()
        s_count = (needs_coverage[col] == 'S').sum()
        tech_summary.append({
            'Technology': tech,
            'Primary': p_count,
            'Secondary': s_count,
            'Total': p_count + s_count
        })

tech_summary_df = pd.DataFrame(tech_summary)

# Stacked bar chart
fig, ax = plt.subplots(figsize=(10, 6))
x = range(len(tech_summary_df))
ax.bar(x, tech_summary_df['Primary'], label='Primary', color='#2e7d32')
ax.bar(x, tech_summary_df['Secondary'], bottom=tech_summary_df['Primary'], 
       label='Secondary', color='#81c784')

ax.set_xticks(x)
ax.set_xticklabels(tech_summary_df['Technology'])
ax.set_ylabel('Number of Asset Classes')
ax.set_xlabel('Technology Code')
ax.set_title('Technology Needs Distribution (Primary vs Secondary)')
ax.legend()
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Print summary table
print("\nTechnology Needs Summary:")
print(tech_summary_df.to_string(index=False))
```
:::


## ASSET SQL PULL

```{python}
#| echo: false
#| label: SQL-asset-class

# Start timing
start_time = time.time()

# Run the sql script
asset_class = run_query_from_file('query/asset-classes.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")

asset_class['ASSETNUM'] = asset_class['ASSETNUM'].astype('category')
asset_class['ASSET_DESC'] = asset_class['ASSET_DESC'].astype('category')
asset_class['ASSET_CLASS'] = asset_class['ASSET_CLASS'].astype('category')
asset_class['ASSET_DEPT'] = asset_class['ASSET_DEPT'].astype('category')

asset_class.info()
```

```{python}
#| echo: false
#| label: output-asset_class-pkl

# Save asset list with asset classes
asset_class.to_pickle('data/asset_class.pkl')
asset_class.to_csv('output/intermediate/asset_class.csv', index=False)

print(f"\nSaved asset list with class information:")
print(f"  - data/asset_class.pkl ({len(asset_class):,} assets)")
print(f"  - output/asset_class.csv")
```

## Merging Needs Coverage with Asset Data
```{python}
#| echo: false
#| label: merge-needs-with-assets

# Merge needs coverage with actual assets
assets_with_needs = asset_class.merge(
    needs_coverage,
    left_on='ASSET_CLASS',
    right_on='ASSETCLASS',
    how='left'
)

# Fill NaN values (asset classes not in needs coverage) with 'N'
needs_cols = [col for col in assets_with_needs.columns if col.startswith('NEEDS_')]
for col in needs_cols:
    assets_with_needs[col] = assets_with_needs[col].fillna('N')

print(f"Total assets: {len(assets_with_needs):,}")
print(f"Asset classes with defined needs: {assets_with_needs['ASSETCLASS'].notna().sum():,}")
print(f"Asset classes without defined needs: {assets_with_needs['ASSETCLASS'].isna().sum():,}")

assets_with_needs.info()
```

```{python}
#| echo: false
#| label: output-assets_with_needs

# Save assets_with_needs
assets_with_needs.to_pickle('data/assets_with_needs.pkl')
assets_with_needs.to_csv('output/intermediate/assets_with_needs.csv', index=False)

print(f"\nSaved final coverage report:")
print(f"  - data/assets_with_needs.pkl ({len(assets_with_needs):,} assets)")
print(f"  - output/assets_with_needs.csv")
```

## Needs by Department

```{python}
#| echo: false
#| label: needs-by-department

# Summary by department - count assets that NEED each technology (P or S)
dept_needs_summary = []

for dept in sorted(assets_with_needs['ASSET_DEPT'].unique()):
    dept_assets = assets_with_needs[assets_with_needs['ASSET_DEPT'] == dept]
    
    record = {
        'Department': dept,
        'Total_Assets': len(dept_assets)
    }
    
    # Count assets needing each technology (Primary or Secondary)
    for tech in tech_cols:
        col = f'NEEDS_{tech}'
        if col in dept_assets.columns:
            needs_count = ((dept_assets[col] == 'P') | (dept_assets[col] == 'S')).sum()
            record[f'{tech}_Needs'] = needs_count
    
    dept_needs_summary.append(record)

dept_needs_df = pd.DataFrame(dept_needs_summary)

print("\nAssets Needing Monitoring by Department:")
print(dept_needs_df.to_string(index=False))
```

## Needs by Department - Primary Only

```{python}
#| echo: false
#| label: needs-by-dept-primary-only

# Alternative view: PRIMARY needs only
dept_needs_primary = []

for dept in sorted(assets_with_needs['ASSET_DEPT'].unique()):
    dept_assets = assets_with_needs[assets_with_needs['ASSET_DEPT'] == dept]
    
    record = {
        'Department': dept,
        'Total_Assets': len(dept_assets)
    }
    
    # Count assets with PRIMARY needs only
    for tech in tech_cols:
        col = f'NEEDS_{tech}'
        if col in dept_assets.columns:
            primary_count = (dept_assets[col] == 'P').sum()
            record[f'{tech}_Primary'] = primary_count
    
    dept_needs_primary.append(record)

dept_primary_df = pd.DataFrame(dept_needs_primary)

print("\nAssets with PRIMARY Technology Needs by Department:")
print(dept_primary_df.to_string(index=False))
```



# Phase II - HAS Monitoring (Routes) {background-color="#7c2d12"}

## ROUTES SQL PULL

```{python}
#| echo: false
#| label: SQL-has-monitoring-routes

# Start timing
start_time = time.time()

# Run the sql script
has_mon_r = run_query_from_file('query/has_mon-routes.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")

has_mon_r['ROUTE'] = has_mon_r['ROUTE'].astype('category')
has_mon_r['ASSETNUM'] = has_mon_r['ASSETNUM'].astype('category')
has_mon_r['CLASS'] = has_mon_r['CLASS'].astype('category')
has_mon_r['DEPT'] = has_mon_r['DEPT'].astype('category')


has_mon_r.info()

#Memory Usage
print(f"Memory usage: {has_mon_r.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

```{python}
#| echo: false
#| label: route-parse-codes

# Step 1: Parse the route description to extract technology code
# Pattern: {DEPT}_{TECH}_{SUBTYPE} - Description
def extract_tech_from_route(route_desc):
    """
    Extract technology code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> 'UL'
    """
    # Better null check for pandas
    if not isinstance(route_desc, str):
        return None
    
    # Match pattern: XXX_YY_ZZZ where YY is the tech code
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        tech = match.group(2)  # Get the second part (tech code)
        return tech
    return None

def extract_dept_from_route(route_desc):
    """
    Extract department code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> '2PA'
    """
    if not isinstance(route_desc, str):
        return None
    
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        return match.group(1)  # First part is DEPT
    return None

def extract_vendor_from_route(route_desc):
    """
    Extract vendor code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> 'UEDMS'
    """
    if not isinstance(route_desc, str):
        return None
    
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        return match.group(3)  # Third part is VENDOR
    return None
```

## Route Code Parsing

```{python}
#| echo: false
#| label: route-apply-code-parsing
#
# Cell 4: Apply the function to create TECH column
has_mon_r['TECH'] = has_mon_r['ROUTE_DESC'].apply(extract_tech_from_route)
has_mon_r['ROUTE_DEPT'] = has_mon_r['ROUTE_DESC'].apply(extract_dept_from_route)
has_mon_r['VENDOR'] = has_mon_r['ROUTE_DESC'].apply(extract_vendor_from_route)

# Look at results
has_mon_r[['ROUTE', 'ROUTE_DESC', 'ROUTE_DEPT', 'TECH', 'VENDOR']].drop_duplicates('ROUTE').head(10)
```

## Route Application Summary

```{python}
#| echo: false
#| label: route-asset-level-summary

# OUTPUT 1: Asset-level summary (one row per asset)
# This keeps the existing structure with Y/N columns
asset_route_tech = has_mon_r[has_mon_r['TECH'].notna()].groupby('ASSETNUM', observed=False)['TECH'].apply(list).reset_index()
asset_route_tech.columns = ['ASSETNUM', 'TECHNOLOGIES']

# Get asset details
asset_route_details = has_mon_r.groupby('ASSETNUM', observed=False).first()[['ASSET_DESC', 'CLASS', 'DEPT']].reset_index()

# Merge together for summary view
asset_route_coverage = asset_route_details.merge(asset_route_tech, on='ASSETNUM', how='left')

print(f"Summary view: {len(asset_route_coverage)} unique assets")
```

```{python}
#| echo: false
#| label: route-asset-tech-vendor-detail


# OUTPUT 2: Asset-Tech-Vendor detail (multiple rows per asset)
# This preserves all the granular relationships
asset_route_tech_vendor_detail = has_mon_r[has_mon_r['TECH'].notna()][['ASSETNUM', 'ASSET_DESC', 'CLASS', 'DEPT', 'ROUTE_DEPT', 'TECH', 'VENDOR']].copy()

# Remove duplicates (same asset might be on multiple routes with same tech-vendor combo)
asset_route_tech_vendor_detail = asset_route_tech_vendor_detail.drop_duplicates(subset=['ASSETNUM', 'TECH', 'VENDOR'])

# Sort for readability
asset_route_tech_vendor_detail = asset_route_tech_vendor_detail.sort_values(['DEPT', 'TECH', 'VENDOR', 'ASSETNUM']).reset_index(drop=True)


print(f"Detail view: {len(asset_route_tech_vendor_detail)} technology to asset combinations")
```

## Add HAS_TECH Column 

```{python}
#| echo: false
#| label: route-add-has-columns

# Get all techs EXCEPT GM (since it comes from meters, not routes)
route_techs = [tech for tech in tech_cols if tech not in ['GM']]

# Cell 7: Create HAS columns (Y/N for each technology) - summary view only
# Note: GM (General Metering) comes from a separate SQL Meters query, not included here
for tech in route_techs:  # Removed 'GM'
    col_name = f'HAS_{tech}'
    asset_route_coverage[col_name] = asset_route_coverage['TECHNOLOGIES'].apply(
        lambda x: 'Y' if isinstance(x, list) and tech in x else 'N'
    )

# Drop the temporary list column
asset_route_coverage = asset_route_coverage.drop('TECHNOLOGIES', axis=1)

# Show summary stats
print("\nAsset Coverage Summary (Route-based monitoring):")
total = len(asset_route_coverage)
for tech in route_techs:  
    count = (asset_route_coverage[f'HAS_{tech}'] == 'Y').sum()
    print(f"{tech}: {count} assets ({count/total*100:.1f}%)")
print("\nNote: GM (General Metering) data comes from separate SQL Meters query")
```

```{python}
#| echo: false
#| label: output-asset_route_coverage

# Save asset_route_coverage
asset_route_coverage.to_pickle('data/asset_route_coverage.pkl')
asset_route_coverage.to_csv('output/intermediate/asset_route_coverage.csv', index=False)

print(f"\nSaved asset_route_coverage:")
print(f"  - data/asset_route_coverage.pkl ({len(asset_route_coverage):,} assets)")
print(f"  - output/intermediate/asset_route_coverage.csv")
```

## Data Architecture

```{mermaid}
%%| label: mermaid-target-OUT
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB1,DB2,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#e8f5e9
    classDef output fill:#6aca8eff
```

::: {.notes}
Three pathways converge: disconnected databases, connectable databases, and native Maximo meters
:::

# PHASE III - HAS Monitoring (Meters) {background-color="#7c2d12"}

## METER SQL PULL

```{python}
#| echo: false
#| label: SQL-has-monitoring-meters

# Start timing
start_time = time.time()

# Run the sql script
has_mon_m = run_query_from_file('query/has_mon-meters.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")


# Primary Conversions
has_mon_m['METERTYPE'] = has_mon_m['METERTYPE'].astype('category')
has_mon_m['ASSETNUM'] = has_mon_m['ASSETNUM'].astype('category')
has_mon_m['METERNAME'] = has_mon_m['METERNAME'].astype('category')
has_mon_m['LASTREADINGINSPCTR'] = has_mon_m['LASTREADINGINSPCTR'].astype('category')
has_mon_m['AVGCALCMETHOD'] = has_mon_m['AVGCALCMETHOD'].astype('category')
has_mon_m['POINTNUM'] = has_mon_m['POINTNUM'].astype('category')


#Memory Usage
print(f"Memory usage: {has_mon_m.memory_usage(deep=True).sum() / 1024**2:.2f} MB")


has_mon_m.info()
```

## Isolate NULL Last Reading Date

```{python}
#| echo: false
#| label: meters-isolate-null

# Isolate records with null LASTREADING_DATE
has_mon_m_flagged = has_mon_m[has_mon_m['LASTREADING_DATE'].isna()].copy()

print(f"Flagged records (null LASTREADING_DATE): {len(has_mon_m):,}")
print(f"Percentage of total: {len(has_mon_m_flagged)/len(has_mon_m)*100:.1f}%")

# Show sample of flagged records
print("\nSample of flagged records:")
has_mon_m_flagged.head()
```

## Aggregate to Asset Level

```{python}
#| echo: false
#| label: meters-aggregate-to-asset-level

# Aggregate to ASSETNUM level
has_mon_m_agg = has_mon_m.groupby('ASSETNUM', observed=False).agg({
    'METERNAME': 'count',  # Count of meters
    'LASTREADING_DATE': 'max',  # Most recent reading date
    'ASSET_DESC': 'first',  # Get asset description (should be same for all records)
    'CLASS': 'first',
    'DEPT': 'first'
}).reset_index()

# Rename columns for clarity
has_mon_m_agg.columns = ['ASSETNUM', 'METER_COUNT', 'MAX_LASTREADING_DATE', 'ASSET_DESC','DEPT','CLASS']

print(f"Total unique assets: {len(has_mon_m_agg):,}")
print(f"\nAggregated data sample:")
has_mon_m_agg.info()
```

## Add 1 year Threshold

```{python}
#| echo: false
#| label: meters-add-last-reading-in-a-year

# Calculate date threshold (1 year ago from today)
one_year_ago = datetime.now() - timedelta(days=365)
print(f"One year ago threshold: {one_year_ago.date()}")

# Add "Last Reading Within 1 Year" column
has_mon_m_agg['READING_WITHIN_1YR'] = has_mon_m_agg['MAX_LASTREADING_DATE'].apply(
    lambda x: 'Y' if pd.notna(x) and x >= one_year_ago else 'N'
)

# Count and display results
reading_counts = has_mon_m_agg['READING_WITHIN_1YR'].value_counts()
print(f"\nReading within 1 year distribution:")
print(reading_counts)
print(f"\nPercentage with recent readings: {reading_counts.get('Y', 0)/len(has_mon_m_agg)*100:.1f}%")

has_mon_m_agg.info()
```

## Add HAS GM column

```{python}
#| echo: false
#| label: meters-add-HAS_GM-col

# HAS_GM is only 'Y' if READING_WITHIN_1YR is 'Y'
# This follows the logic: "if this is Y... then another column (same calculation) HAS_GM = Y/N"
has_mon_m_agg['HAS_GM'] = has_mon_m_agg['READING_WITHIN_1YR'].apply(
    lambda x: 'Y' if x == 'Y' else 'N'
)

# Display results
print("Distribution of HAS_GM:")
print(has_mon_m_agg['HAS_GM'].value_counts())

# Cross-tabulation to verify logic
print("\nCross-tabulation (READING_WITHIN_1YR vs HAS_GM):")
print(pd.crosstab(has_mon_m_agg['READING_WITHIN_1YR'], has_mon_m_agg['HAS_GM']))

print("\nFinal aggregated data sample:")
has_mon_m_agg.info()
```

```{python}
#| echo: false
#| label: output-has_mon_m_agg

# Save has_mon_m_agg
has_mon_m_agg.to_pickle('data/has_mon_m_agg.pkl')
has_mon_m_agg.to_csv('output/intermediate/has_mon_m_agg.csv', index=False)

print(f"\nSaved final coverage report:")
print(f"  - data/has_mon_m_agg.pkl ({len(has_mon_m_agg):,} assets)")
print(f"  - output/intermediate/has_mon_m_agg.csv")
```

## General Meters Summary Stats

```{python}
#| echo: false
#| label: meters-summary-stats

print("=" * 60)
print("SUMMARY STATISTICS")
print("=" * 60)

print(f"\nTotal Assets: {len(has_mon_m_agg):,}")
print(f"Total Meters: {has_mon_m_agg['METER_COUNT'].sum():,}")
print(f"Average Meters per Asset: {has_mon_m_agg['METER_COUNT'].mean():.2f}")
print(f"Median Meters per Asset: {has_mon_m_agg['METER_COUNT'].median():.0f}")
print(f"Max Meters on Single Asset: {has_mon_m_agg['METER_COUNT'].max()}")

print(f"\nAssets with Recent Readings (within 1 year): {(has_mon_m_agg['READING_WITHIN_1YR'] == 'Y').sum():,}")
print(f"Assets with HAS_GM = Y: {(has_mon_m_agg['HAS_GM'] == 'Y').sum():,}")
print(f"Assets with no recent readings: {(has_mon_m_agg['READING_WITHIN_1YR'] == 'N').sum():,}")

print(f"\nAssets with null MAX_LASTREADING_DATE: {has_mon_m_agg['MAX_LASTREADING_DATE'].isna().sum():,}")
```

# PHASE IV -  HAS Merging

## Combine Route and Meter Coverage
```{python}
#| echo: true
#| label: merge-routes-meters-coverage

# Drop asset attributes from both coverage tables (we'll get from asset_class)
asset_route_coverage_slim = asset_route_coverage.drop(['ASSET_DESC', 'CLASS', 'DEPT'], axis=1)
has_mon_m_agg_slim = has_mon_m_agg.drop(['ASSET_DESC', 'CLASS', 'DEPT'], axis=1)

# Merge route and meter coverage (both are just ASSETNUM + flags now)
has_monitoring_slim = asset_route_coverage_slim.merge(
    has_mon_m_agg_slim[['ASSETNUM', 'HAS_GM', 'METER_COUNT', 'MAX_LASTREADING_DATE']],
    on='ASSETNUM',
    how='outer'
)

# Now merge with asset_class to get the authoritative asset attributes
has_monitoring = has_monitoring_slim.merge(
    asset_class,
    on='ASSETNUM',
    how='left'  # Left join because we want all monitored assets
)

# Fill NaN values with 'N' for technologies not present
has_cols = [col for col in has_monitoring.columns if col.startswith('HAS_')]
for col in has_cols:
    has_monitoring[col] = has_monitoring[col].fillna('N')

print(f"Total assets with monitoring: {len(has_monitoring):,}")
print(f"Assets matched to asset master: {has_monitoring['ASSET_CLASS'].notna().sum():,}")
print(f"Assets NOT in asset master: {has_monitoring['ASSET_CLASS'].isna().sum():,}")

has_monitoring.info()
```

```{python}
#| echo: false
#| label: output-has_monitoring

# Save has_monitoring
has_monitoring.to_pickle('data/has_monitoring.pkl')
has_monitoring.to_csv('output/intermediate/has_monitoring.csv', index=False)

print(f"\nSaved has_monitoring:")
print(f"  - data/has_monitoring.pkl ({len(has_monitoring):,} assets)")
print(f"  - output/intermediate/has_monitoring.csv")
```

```{python}
#| echo: true
#| label: has-monitoring-summary

# Summary statistics across all technologies
print("\nHAS Monitoring Coverage Summary:")
total = len(has_monitoring)
for tech in tech_cols:
    col = f'HAS_{tech}'
    if col in has_monitoring.columns:
        count = (has_monitoring[col] == 'Y').sum()
        print(f"{tech}: {count:,} assets ({count/total*100:.1f}% of monitored assets)")
```

# PHASE V - Coverage Report: Needs vs Has

```{python}
#| echo: true
#| label: merge-needs-has-coverage

# Merge has_monitoring with assets_with_needs (just the NEEDS columns)
needs_cols = [col for col in assets_with_needs.columns if col.startswith('NEEDS_')]
coverage_report = has_monitoring.merge(
    assets_with_needs[['ASSETNUM'] + needs_cols],
    on='ASSETNUM',
    how='outer'
)

# Now merge with asset_class to get complete asset attributes for ALL assets
coverage_report = coverage_report.merge(
    asset_class[['ASSETNUM', 'ASSET_DESC', 'ASSET_CLASS', 'ASSET_DEPT']],
    on='ASSETNUM',
    how='left',
    suffixes=('_old', '')
)

# Drop the old columns that had NaN
coverage_report = coverage_report.drop(['ASSET_DESC_old', 'ASSET_CLASS_old', 'ASSET_DEPT_old'], 
                                       axis=1, errors='ignore')

# Fill any missing NEEDS or HAS flags with 'N'
for col in coverage_report.columns:
    if col.startswith('NEEDS_') or col.startswith('HAS_'):
        coverage_report[col] = coverage_report[col].fillna('N')

print(f"Total assets in coverage report: {len(coverage_report):,}")
coverage_report.info()
```

## Coverage Judgment Logic
```{python}
#| echo: true
#| label: calculate-coverage-judgment

# Create judgment columns for each technology
for tech in tech_cols:
    needs_col = f'NEEDS_{tech}'
    has_col = f'HAS_{tech}'
    judge_col = f'{tech}_JUDGE'
    
    # Apply quad-state judgment logic
    def judge_coverage(row):
        need = row[needs_col]
        has = row[has_col]
        
        # Critical Gap: Has P or S need but NO monitoring
        if need in ['P', 'S'] and has == 'N':
            return 'Critical Gap'
        
        # Compliant: Has P need and HAS monitoring
        if need == 'P' and has == 'Y':
            return 'Compliant'
        
        # Partial Coverage: Has S need and monitoring, OR has monitoring but no need
        if (need == 'S' and has == 'Y') or (need == 'N' and has == 'Y'):
            return 'Partial Coverage'
        
        # Not Applicable: No need (N) and no monitoring
        if need == 'N' and has == 'N':
            return 'Not Applicable'
        
        # Fallback (shouldn't hit this)
        return 'Unknown'
    
    coverage_report[judge_col] = coverage_report.apply(judge_coverage, axis=1)

# Create MASTER_JUDGE
judge_cols = [f'{tech}_JUDGE' for tech in tech_cols]

def master_judgment(row):
    judgments = row[judge_cols].values
    
    # Critical Gap: ANY technology has critical gap
    if 'Critical Gap' in judgments:
        return 'Critical Gap'
    
    # Compliant: At least one Compliant AND no critical gaps AND no partial
    if 'Compliant' in judgments and 'Partial Coverage' not in judgments:
        return 'Compliant'
    
    # Partial Coverage: Has some coverage but not full compliance
    if 'Compliant' in judgments or 'Partial Coverage' in judgments:
        return 'Partial Coverage'
    
    # Not Applicable: All technologies are N/A
    if all(j == 'Not Applicable' for j in judgments):
        return 'Not Applicable'
    
    return 'Unknown'

coverage_report['MASTER_JUDGE'] = coverage_report[judge_cols].apply(master_judgment, axis=1)

# Summary statistics
print("Coverage Judgment Summary:")
for status in ['Compliant', 'Partial Coverage', 'Critical Gap', 'Not Applicable']:
    count = (coverage_report['MASTER_JUDGE'] == status).sum()
    print(f"{status}: {count:,} assets")

coverage_report.info()
```

```{python}
#| echo: false
#| label: output-coverage-pkl

# Save final coverage report
coverage_report.to_pickle('data/coverage_report.pkl')
coverage_report.to_csv('output/coverage_report.csv', index=False)

print(f"\nSaved final coverage report:")
print(f"  - data/coverage_report.pkl ({len(coverage_report):,} assets)")
print(f"  - output/coverage_report.csv")
```

## Connectable Databases {.smaller}

::: {.columns}
::: {.column width="60%"}

```{mermaid}
%%| label: mermaid-target-DB1
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB2,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class DB1 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:3px,color:#ffffff
```
:::


::: {.column width="40%"}
**API-Enabled Systems**

- Waites (Vibration)
- Hydac (Lubrication)
- FLUKE (IR/Other)
- ZDT (Fanuc)

:::
:::

::: {.aside}
If API not available, use Disconnected Database approach
:::

## Disconnected Databases {.smaller}

::: {.columns}
::: {.column width="60%"}
```{mermaid}
%%| label: mermaid-target-DB2
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB1,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class DB2 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:4px,color:#ffffff
```
:::

::: {.column width="40%"}
**Manual Route Coding**

- FLIR (IR)
- MOTION (Vibration)
- ONTrak (Ultrasound)
- AllTest (Motor Current)
- LubeCon (Chain Monitoring)

:::
:::

::: {.fragment}
**Solution:** Standard naming conventions from MESD-ACM-0001
:::

## Route Naming Standard {.smaller}

::: {.columns}
::: {.column width="50%"}
**Standard Convention** (from MESD-ACM-0001)

![Standard Route Coding](/img/acm003_00.png)

Format: `DEPT_TECH_VENDOR - Description`
:::

::: {.column width="50%"}
**Maximo Examples**

![Examples in Maximo](/img/acm003_01.png)

::: {.fragment}
Two underscores + space-space separator
:::
:::
:::

## SQL Routes Query {.smaller}

```{mermaid}
%%| label: has-monitoring-SQL-routes
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB2,DB1,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class SQL1 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:4px,color:#ffffff
```

### Pattern Matching Logic

```sql
WHERE REGEXP_LIKE(ROUTE.DESCRIPTION, '^[A-Z0-9]+_[A-Z0-9]+_[A-Z0-9]+ - .+') 
```

Matches: **THREE_PART_CODE - Some text description**


::: {.aside}
Can be used in Maximo WHERE clause for saved queries
:::

## Complete Routes Query {.smaller .scrollable}

```{python}
#| echo: false
#| output: asis
#| label: routes-query

import os

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    with open(os.path.join(data_dir, "acm_assets_routes-coverage.sql"), 'r') as f:
        print(f"```sql\n{f.read()}\n```")
except FileNotFoundError:
    print("```sql\n-- SQL file not found in presentation environment\n-- See documentation for full query\n```")
```

## Routes Query Output {.smaller}

![DBeaver SQL Routes Output](/img/acm003_08.png)

::: {.fragment}
**Next Steps:** Transform into Technology & Vendor columns
:::

## Routes Transformation {auto-animate=true}

### Raw Output → Tech-Vendor Detail

![Tech-Vendor Detail](/img/acm003_10.png)

## Routes Transformation {auto-animate=true}

### Tech-Vendor Detail → Coverage Summary

![Coverage Summary](/img/acm003_09.png)

::: {.fragment}
Pivot to show each asset's technology coverage (Y/N flags)
:::

---

# Routes Coverage Analytics {background-color="#4c1d95"}

## {background-image=".\img\acm003_11.png" background-size="contain"}

::: {.notes}
Technology Coverage by Department
:::

## {background-image=".\img\acm003_12.png" background-size="contain"}

::: {.notes}
Flip perspective: how departments are covered across technologies
:::

---

# Meter-Based Monitoring {background-color="#0c4a6e"}

## Native Maximo Approach {.smaller}

::: {.columns}
::: {.column width="50%"}
![Native Maximo Flow](/img/acm003_06.png)
:::

::: {.column width="50%"}
**Direct Meter Records**

::: {.incremental}
- Readings stored in Maximo
- Direct SQL access
- Simpler than route parsing
- Native asset linkage
:::
:::
:::

## SQL Meters Query {.smaller}

![SQL Meters Flow](/img/acm003_07.png)

**Extract:** Meter Name, Last Reading Date, Asset Number

## Complete Meters Query {.smaller .scrollable}

```{python}
#| echo: false
#| output: asis
#| label: meters-query

import os

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    with open(os.path.join(data_dir, "acm_assets_meters-coverage.sql"), 'r') as f:
        print(f"```sql\n{f.read()}\n```")
except FileNotFoundError:
    print("```sql\n-- SQL file not found in presentation environment\n-- See documentation for full query\n```")
```

## Meters Query Results {.smaller}

![Meters Coverage Raw](/img/acm003_13.png)

::: {.fragment}
**Scale:** 24,091 meters applied to 8,223 assets
:::

## Data Quality Assessment {.smaller}

### Three Key Questions

::: {.incremental}
1. Which meters are **active**?
2. How many meters per asset?
3. Does asset qualify as "Has General Metering"?
:::

## Missing Data Analysis {.smaller .scrollable}

| Field | Missing Count | Percentage |
|-------|--------------|------------|
| ASSETNUM | 0 | 0.00% |
| METERNAME | 0 | 0.00% |
| METERTYPE | 0 | 0.00% |
| AVGCALCMETHOD | 19,488 | 80.89% |
| ROLLDOWNSOURCE | 19,570 | 81.23% |
| POINTNUM | 15,171 | 62.97% |
| **LASTREADING** | **5,679** | **23.57%** |
| LASTREADING_DATE | 5,568 | 23.11% |

::: {.fragment}
**Key Finding:** ~23% of meters have no reading date
:::

::: {.notes}
Available for investigation: meters_flagged_null_readings.csv
:::

## Activity Threshold Decision {.smaller}

**Question:** How old can last reading be before asset loses coverage?

::: {.fragment}
```{mermaid}
%%| fig-width: 8
flowchart LR
    A{Within 1 Year?}
    A -->|Yes| B[Has General Meter - Y]
    A -->|No| C[None - N]
    
    style B fill:#e8f5e9
    style C fill:#ffebee
```
:::

::: {.fragment}
**Decision:** 1-year threshold for active monitoring
:::

## Asset-Level Aggregation {.smaller}

![Meter Count Aggregated to Asset](/img/acm003_14.png)

::: {.incremental}
- **Count** of meters per asset
- **Max** last reading date
- Basis for coverage flag
:::

## Coverage Classification {.smaller}

![HAS_GM Column Added](/img/acm003_15.png)

::: {.columns}
::: {.column width="50%"}
**New Columns**

- `READING_WITHIN_1YR`
- `HAS_GM` (Y/N flag)
:::

::: {.column width="50%"}
::: {.fragment}
**Logic:** If max reading within 1 year → Y, else N
:::
:::
:::

---

# Data Integration {background-color="#831843"}

## Python Merge Strategy {.smaller}

::: {.callout-important}
## Documentation In Progress
Detailed merge logic being finalized
:::

::: {.fragment}
**Approach:** Pre-aggregation merge using asset master as single source of truth
:::

## Initial Coverage Results {.smaller}

| Technology | Coverage | Percentage |
|-----------|----------|------------|
| **GM** (General Metering) | 5,005 assets | 60.9% |
| **IR** (Infrared) | 360 assets | 4.4% |
| **UL** (Ultrasound) | 207 assets | 2.5% |
| **VI** (Vibration) | 345 assets | 4.2% |
| **LU** (Lubrication) | 131 assets | 1.6% |
| **MC** (Motor Circuit) | 0 assets | 0.0% |
| **ZD** (Zero Defect) | 86 assets | 1.0% |

::: {.fragment}
**Total Asset Base:** ~8,200 assets
:::

---

# Needs Monitoring {background-color="#713f12"}

## The Complexity Challenge {.smaller}

::: {.incremental}
- Each technology monitors specific **components**
- Each technology targets specific **failure modes**
- Maximo lacks **component-level granularity**
- Need systematic mapping approach
:::

## Solution: Component Mapping {.smaller}

**Reference:** Draft 001 - ACM Technology Application (Table 2)

::: {.fragment}
**Legend:**
- **P** = Primary application
- **S** = Secondary application
:::

## Component to Technology Map {.smaller .scrollable}

```{python}
#| echo: false
#| label: component-map
#| tbl-cap: Component to Technology Map

import pandas as pd

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    component_map = pd.read_csv(f"{data_dir}/component_map.csv")
    component_map.style.hide(axis='index')
except FileNotFoundError:
    # Create sample structure if file not found
    sample_data = {
        'Component': ['Bearings', 'Motors', 'Gearboxes', 'Hydraulics'],
        'VI': ['P', 'P', 'P', 'S'],
        'IR': ['S', 'P', 'S', 'P'],
        'UL': ['P', 'S', 'S', 'P'],
        'LU': ['P', 'S', 'P', 'S']
    }
    pd.DataFrame(sample_data).style.hide(axis='index')
```

---

# Next Steps {background-color="#1e3a8a"}

## Coverage Gap Analysis {.smaller}

::: {.incremental}
1. Map asset types to required technologies
2. Compare needs vs actual coverage
3. Identify critical monitoring gaps
4. Prioritize implementation plan
:::

## Documentation & Resources {.smaller}

::: {.columns}
::: {.column width="50%"}
**Related Resources**
- MESD-ACM-0001 Standard
- Draft 001 Tech Specification
- SQL query repository
- Python merge scripts
:::

::: {.column width="50%"}
**Key Contacts**
- Stakeholder: [TBD]
- Team Members: [TBD]
- Technical Support: [TBD]
:::
:::

---

# Questions? {background-color="#1e3a8a"}

## Thank You

**Feedback & Discussion**
