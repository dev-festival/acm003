---
title: "AAP ACM Coverage to Standard"
subtitle: "Implementation of Standards"
author: "Mike"
date: today
format:
  revealjs:
    theme: [default]
    slide-number: true
    embed-resources: true
    preview-links: auto
    navigation-mode: linear
    controls: true
    progress: true
    history: true
    center: true
    transition: slide
    background-transition: fade
    width: 1280
    height: 1080
    incremental: false
    smaller: true
    scrollable: true
    df-print: kable # or 'paged' or 'default'
execute:
  echo: false
  warning: false
---

```{python}
#| echo: false
#| label: import-libraries

import pandas as pd
import numpy as np
import sys
import os
import pyodbc
import re
from dotenv import load_dotenv, find_dotenv
from pathlib import Path
import warnings
import time       # for timing longer cells (running the sql query)
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime,timedelta
from IPython.display import Markdown
```

DB Secondary : MAS PROD  
   Server name        : ahmplaw2masdb02.aws.honda.com	
   Port number        : 51046	
   Authentication   : SERVER_ENCRYPT	
   Database name : MASDBPRD	
USERID : masro
Password : Stew@506

```{python}
#| echo: false
#| label: ODBC-credentials

# Automatically search for .env in parent directories
load_dotenv(find_dotenv())

# Load from environment variables
DSN = os.getenv('MAXIMO_DSN')
USER = os.getenv('MAXIMO_USER')
PASSWORD = os.getenv('MAXIMO_PASS')

DSN = 'MAS9'
USER = 'masro'
PASSWORD = 'Stew@506' 

#Confirm Credentials
print(f"DSN: {DSN}")
print(f"User: {USER}")
```

```{python}
#| echo: false
#| label: query-function

# Suppress the pandas warning
warnings.filterwarnings('ignore', category=UserWarning)

def run_query_from_file(sql_path: str) -> pd.DataFrame:
    conn = pyodbc.connect(f"DSN={DSN};UID={USER};PWD={PASSWORD}")
    query = open(sql_path).read()
    df = pd.read_sql(query, conn)
    conn.close()
    return df
```

# Phase 0 - Introduction {background-color="#1e3a8a"}

## Project Overview {.smaller}

Demonstrate implementation of standards from:

::: {.columns}
::: {.column width="60%"}
- [MESD-ACM-0001](https://globalhonda.sharepoint.com/:w:/r/sites/gna00880-08-Reliability/Shared%20Documents/08-%20Reliability/04-ACM/00_Overall%20Documents/05_Frameworks/000%20-%20Standard%20for%20Tracking%20ACM%20Technology%20Application.docx?d=w9b9e0e5d7f2c4fdbb8f5ee2ce4919adb&csf=1&web=1&e=uMqNtS) - Standard for Tracking (Has Monitoring)
- [Draft 001](https://globalhonda.sharepoint.com/:w:/r/sites/gna00880-08-Reliability/Shared%20Documents/08-%20Reliability/04-ACM/00_Overall%20Documents/05_Frameworks/001%20-%20Asset%20Condition%20Monitoring%20Technology%20Application%20Specification.docx?d=wb984d67b7b004337aa6b469a06190d85&csf=1&web=1&e=JHKuoG) - Technology Application (Needs Monitoring)


**Goal:** Compare Coverage Needs vs Coverage Actuals
:::

:::{.column width ="40%"}
```{mermaid}
%%| label: mermaid-coverage-report

%%| fig-width: 8
flowchart LR
    A[Has Monitoring] --> C[Coverage Report]
    B[Needs Monitoring] --> C
    
    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#fff4e1
```

::: {.notes}
Two-part analysis: what we NEED to monitor vs what we ARE monitoring
:::

:::

:::

{background-color="#6aca8eff"}

## Technology Definitions 

Technologies are defined in MESD-ACM-001 as: 


## Data Sources Overview

1. **Has Monitoring:** Uses the MESD-ACM-001 Standard for how to pull that information
    - Disconnected Database: `Maximo` ODBC connection to pull Routes & Meters for "Has Monitoring"
    - Connected Database: `API` python connection
    - Some databases are *Connectable* but do not have active connections yet
2. **Needs Monitoring:** Uses the MESD-ACM-002 Standard for how to define what needs monitoring
    - Asset Classes are pulled from `Maximo` using ODBC connection 
    - Technologies are mapped to components from `comp_xref_tech.csv`
    - Components are mapped to asset classes from `class_xref_comp.csv`


# Phase I - Needs Monitoring {background-color="#15803d"}

```{python}
#| label: build-needs-coverage

# Load normalized config files directly
config_dir = 'data/st_tbl/normalized_config'

config = {
    'components': pd.read_csv(f'{config_dir}/components.csv'),
    'technologies': pd.read_csv(f'{config_dir}/technologies.csv'),
    'classes': pd.read_csv(f'{config_dir}/classes.csv'),
    'component_technology': pd.read_csv(f'{config_dir}/component_technology.csv'),
    'class_component': pd.read_csv(f'{config_dir}/class_component.csv')
}

# Build needs coverage using natural keys (post-migration)
needs_coverage = (
    config['class_component']
    .merge(
        config['component_technology'][['component_name', 'technology_code', 'application_type']], 
        on='component_name'
    )
    [['class_name', 'component_name', 'technology_code', 'application_type']]
    .rename(columns={'class_name': 'ASSET_CLASS', 'technology_code': 'TECH'})
)

print(f"✓ Built needs coverage: {len(needs_coverage)} class-component-technology mappings")
```

```{python}
#| label: create-tech-cols

tech_cols = config['technologies']['technology_code'].unique().tolist()


print(f"✓ Working with: {tech_cols}")
```

```{python}
#| label: create-needs-flags

# NEEDS flags: Only Primary applications
needs_primary = needs_coverage[needs_coverage['application_type'] == 'Primary'].copy()

class_tech_needs = (
    needs_primary[['ASSET_CLASS', 'TECH']]
    .drop_duplicates()
)

# Pivot to NEEDS_IR, NEEDS_VI, etc.
tech_needs_wide = (
    class_tech_needs
    .assign(value='Y')
    .pivot_table(
        index='ASSET_CLASS',
        columns='TECH',
        values='value',
        fill_value='N',
        aggfunc='first'
    )
    .add_prefix('NEEDS_')
    .reset_index()
)

# Ensure all expected technologies exist (even if not in config yet)
for tech in tech_cols:
    col_name = f'NEEDS_{tech}'
    if col_name not in tech_needs_wide.columns:
        tech_needs_wide[col_name] = 'N'
        print(f"⚠️  Added missing column: {col_name} (not configured yet)")

print(f"✓ Created NEEDS flags for {len(tech_needs_wide.columns)-1} technologies")
print(f"✓ Based on PRIMARY applications only")
tech_needs_wide.head()
```

```{python}
#| label: create-use-flags

# USE flags: Both Primary AND Secondary applications
class_tech_use = (
    needs_coverage[['ASSET_CLASS', 'TECH']]
    .drop_duplicates()
)

# Pivot to USE_IR, USE_VI, etc.
tech_use_wide = (
    class_tech_use
    .assign(value='Y')
    .pivot_table(
        index='ASSET_CLASS',
        columns='TECH',
        values='value',
        fill_value='N',
        aggfunc='first'
    )
    .add_prefix('USE_')
    .reset_index()
)

# Ensure all expected technologies exist

for tech in tech_cols:
    col_name = f'USE_{tech}'
    if col_name not in tech_use_wide.columns:
        tech_use_wide[col_name] = 'N'
        print(f"⚠️  Added missing column: {col_name} (not configured yet)")

print(f"✓ Created USE flags for {len(tech_use_wide.columns)-1} technologies")
print(f"✓ Based on PRIMARY + SECONDARY applications")
tech_use_wide.head()

```

## Definition {.smaller}

 What assets *should* have monitoring based on their class/components? 

```{mermaid}
%%| label: mermaid-monitoring-map

flowchart LR
COMPMAP[Component Map to Technology]
CLASSMAP[Asset Class Mapped to Component]
PYTHMERGE[Python Merge]
OUTPUT[Needs Monitoring Map]

COMPMAP --> PYTHMERGE
CLASSMAP --> PYTHMERGE
PYTHMERGE --> OUTPUT
```


## Needs Monitoring: Definition {.smaller}

"Needs Monitoring" is determined by the asset's **class** and **components**.

### Asset Class to Component Mapping {.smaller}

The `class_xref_comp.csv` table cross-references:

- **Asset Classes** (pulled from Maximo) 
- **Components** that each class *should* have, list developed with RE's (inside MESD-ACM-002) [doc ref??]

This establishes the expected component inventory for each asset type.

**Example**: A "Pump" class should have components like:

- Motor
- Bearings
- Coupling
- Seal assembly

## Mapping Logic

1. **Start with Asset Class** (from Maximo)
2. **Identify Required Components** (via `class_xref_comp.csv`)
3. **Determine Required Technologies** (via `comp_xref_tech.csv`)
4. **Result**: Boolean flags for which technologies this asset *needs*

## Mapping Tables {.smaller}

::: panel-tabset
### Class → Component Map

### Component → Tech Map

P = Primary Technology

S = Secondary Technology

:::


## ASSET SQL PULL

```{python}
#| echo: false
#| label: SQL-asset-class

# Start timing
start_time = time.time()

# Run the sql script
asset_class = run_query_from_file('query/asset-classes.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")

asset_class['ASSETNUM'] = asset_class['ASSETNUM'].astype('category')
asset_class['ASSET_DESC'] = asset_class['ASSET_DESC'].astype('category')
asset_class['ASSET_CLASS'] = asset_class['ASSET_CLASS'].astype('category')
asset_class['ASSET_DEPT'] = asset_class['ASSET_DEPT'].astype('category')

asset_class.info()
```

```{python}
#| echo: false
#| label: output-asset_class-pkl

# Save asset list with asset classes
asset_class.to_pickle('data/asset_class.pkl')
asset_class.to_csv('output/intermediate/asset_class.csv', index=False)

print(f"\nSaved asset list with class information:")
print(f"  - data/asset_class.pkl ({len(asset_class):,} assets)")
print(f"  - output/asset_class.csv")
```

## Merging Needs Coverage with Asset Data

```{python}
#| label: merge-needs-use-with-assets

# Merge both NEEDS and USE flags with asset_class
asset_class = asset_class.merge(tech_needs_wide, on='ASSET_CLASS', how='left')
asset_class = asset_class.merge(tech_use_wide, on='ASSET_CLASS', how='left')

# Fill NaN with 'N' for classes not in config
needs_cols = [col for col in asset_class.columns if col.startswith('NEEDS_')]
use_cols = [col for col in asset_class.columns if col.startswith('USE_')]

for col in needs_cols + use_cols:
    asset_class[col] = asset_class[col].fillna('N')

print(f"✓ Assets with NEEDS and USE flags: {len(asset_class):,}")

# Summary comparison
print("\n" + "="*60)
print("NEEDS (Primary only) vs USE (Primary + Secondary)")
print("="*60)

for tech in tech_cols:
    needs_count = (asset_class[f'NEEDS_{tech}'] == 'Y').sum()
    use_count = (asset_class[f'USE_{tech}'] == 'Y').sum()
    print(f"{tech:4s} | NEEDS: {needs_count:6,} | USE: {use_count:6,} | Diff: {use_count-needs_count:6,}")

asset_class.info()
```

# Phase II - HAS Monitoring (Routes) {background-color="#7c2d12"}

## ROUTES SQL PULL

```{python}
#| echo: false
#| label: SQL-has-monitoring-routes

# Start timing
start_time = time.time()

# Run the sql script
has_mon_r = run_query_from_file('query/has_mon-routes.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")

has_mon_r['ROUTE'] = has_mon_r['ROUTE'].astype('category')
has_mon_r['ASSETNUM'] = has_mon_r['ASSETNUM'].astype('category')
has_mon_r['CLASS'] = has_mon_r['CLASS'].astype('category')
has_mon_r['DEPT'] = has_mon_r['DEPT'].astype('category')


has_mon_r.info()

#Memory Usage
print(f"Memory usage: {has_mon_r.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

```{python}
#| echo: false
#| label: route-parse-codes

# Step 1: Parse the route description to extract technology code
# Pattern: {DEPT}_{TECH}_{SUBTYPE} - Description
def extract_tech_from_route(route_desc):
    """
    Extract technology code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> 'UL'
    """
    # Better null check for pandas
    if not isinstance(route_desc, str):
        return None
    
    # Match pattern: XXX_YY_ZZZ where YY is the tech code
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        tech = match.group(2)  # Get the second part (tech code)
        return tech
    return None

def extract_dept_from_route(route_desc):
    """
    Extract department code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> '2PA'
    """
    if not isinstance(route_desc, str):
        return None
    
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        return match.group(1)  # First part is DEPT
    return None

def extract_vendor_from_route(route_desc):
    """
    Extract vendor code from route description
    Example: '2PA_UL_UEDMS - Penthouse Route 1' -> 'UEDMS'
    """
    if not isinstance(route_desc, str):
        return None
    
    match = re.match(r'^([A-Z0-9]+)_([A-Z0-9]+)_([A-Z0-9]+)\s*-', route_desc)
    if match:
        return match.group(3)  # Third part is VENDOR
    return None
```

## Route Code Parsing

```{python}
#| echo: false
#| label: route-apply-code-parsing
#
# Cell 4: Apply the function to create TECH column
has_mon_r['TECH'] = has_mon_r['ROUTE_DESC'].apply(extract_tech_from_route)
has_mon_r['ROUTE_DEPT'] = has_mon_r['ROUTE_DESC'].apply(extract_dept_from_route)
has_mon_r['VENDOR'] = has_mon_r['ROUTE_DESC'].apply(extract_vendor_from_route)

# Look at results
has_mon_r[['ROUTE', 'ROUTE_DESC', 'ROUTE_DEPT', 'TECH', 'VENDOR']].drop_duplicates('ROUTE').head(10)
```

## Route Application Summary

```{python}
#| echo: false
#| label: route-asset-level-summary

# OUTPUT 1: Asset-level summary (one row per asset)
# This keeps the existing structure with Y/N columns
asset_route_tech = has_mon_r[has_mon_r['TECH'].notna()].groupby('ASSETNUM', observed=False)['TECH'].apply(list).reset_index()
asset_route_tech.columns = ['ASSETNUM', 'TECHNOLOGIES']

# Get asset details
asset_route_details = has_mon_r.groupby('ASSETNUM', observed=False).first()[['ASSET_DESC', 'CLASS', 'DEPT']].reset_index()

# Merge together for summary view
asset_route_coverage = asset_route_details.merge(asset_route_tech, on='ASSETNUM', how='left')

print(f"Summary view: {len(asset_route_coverage)} unique assets")
```

```{python}
#| echo: false
#| label: route-asset-tech-vendor-detail


# OUTPUT 2: Asset-Tech-Vendor detail (multiple rows per asset)
# This preserves all the granular relationships
asset_route_tech_vendor_detail = has_mon_r[has_mon_r['TECH'].notna()][['ASSETNUM', 'ASSET_DESC', 'CLASS', 'DEPT', 'ROUTE_DEPT', 'TECH', 'VENDOR']].copy()

# Remove duplicates (same asset might be on multiple routes with same tech-vendor combo)
asset_route_tech_vendor_detail = asset_route_tech_vendor_detail.drop_duplicates(subset=['ASSETNUM', 'TECH', 'VENDOR'])

# Sort for readability
asset_route_tech_vendor_detail = asset_route_tech_vendor_detail.sort_values(['DEPT', 'TECH', 'VENDOR', 'ASSETNUM']).reset_index(drop=True)


print(f"Detail view: {len(asset_route_tech_vendor_detail)} technology to asset combinations")
```

## Add HAS_TECH Column 

```{python}
#| echo: false
#| label: route-add-has-columns

# Get all techs EXCEPT GM (since it comes from meters, not routes)
route_techs = [tech for tech in tech_cols if tech not in ['GM']]

# Cell 7: Create HAS columns (Y/N for each technology) - summary view only
# Note: GM (General Metering) comes from a separate SQL Meters query, not included here
for tech in route_techs:  # Removed 'GM'
    col_name = f'HAS_{tech}'
    asset_route_coverage[col_name] = asset_route_coverage['TECHNOLOGIES'].apply(
        lambda x: 'Y' if isinstance(x, list) and tech in x else 'N'
    )

# Drop the temporary list column
asset_route_coverage = asset_route_coverage.drop('TECHNOLOGIES', axis=1)

# Show summary stats
print("\nAsset Coverage Summary (Route-based monitoring):")
total = len(asset_route_coverage)
for tech in route_techs:  
    count = (asset_route_coverage[f'HAS_{tech}'] == 'Y').sum()
    print(f"{tech}: {count} assets ({count/total*100:.1f}%)")
print("\nNote: GM (General Metering) data comes from separate SQL Meters query")
```

```{python}
#| echo: false
#| label: output-asset_route_coverage

# Save asset_route_coverage
asset_route_coverage.to_pickle('data/asset_route_coverage.pkl')
asset_route_coverage.to_csv('output/intermediate/asset_route_coverage.csv', index=False)

print(f"\nSaved asset_route_coverage:")
print(f"  - data/asset_route_coverage.pkl ({len(asset_route_coverage):,} assets)")
print(f"  - output/intermediate/asset_route_coverage.csv")
```

## Data Architecture

```{mermaid}
%%| label: mermaid-target-OUT
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB1,DB2,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#e8f5e9
    classDef output fill:#6aca8eff
```

::: {.notes}
Three pathways converge: disconnected databases, connectable databases, and native Maximo meters
:::

# PHASE III - HAS Monitoring (Meters) {background-color="#7c2d12"}

## METER SQL PULL

```{python}
#| echo: false
#| label: SQL-has-monitoring-meters

# Start timing
start_time = time.time()

# Run the sql script
has_mon_m = run_query_from_file('query/has_mon-meters.sql')

# Calculate and print execution time
execution_time = time.time() - start_time
print(f"Query execution time: {execution_time:.2f} seconds \n")


# Primary Conversions
has_mon_m['METERTYPE'] = has_mon_m['METERTYPE'].astype('category')
has_mon_m['ASSETNUM'] = has_mon_m['ASSETNUM'].astype('category')
has_mon_m['METERNAME'] = has_mon_m['METERNAME'].astype('category')
has_mon_m['LASTREADINGINSPCTR'] = has_mon_m['LASTREADINGINSPCTR'].astype('category')
has_mon_m['AVGCALCMETHOD'] = has_mon_m['AVGCALCMETHOD'].astype('category')
has_mon_m['POINTNUM'] = has_mon_m['POINTNUM'].astype('category')


#Memory Usage
print(f"Memory usage: {has_mon_m.memory_usage(deep=True).sum() / 1024**2:.2f} MB")


has_mon_m.info()
```

## Isolate NULL Last Reading Date

```{python}
#| echo: false
#| label: meters-isolate-null

# Isolate records with null LASTREADING_DATE
has_mon_m_flagged = has_mon_m[has_mon_m['LASTREADING_DATE'].isna()].copy()

print(f"Flagged records (null LASTREADING_DATE): {len(has_mon_m):,}")
print(f"Percentage of total: {len(has_mon_m_flagged)/len(has_mon_m)*100:.1f}%")

# Show sample of flagged records
print("\nSample of flagged records:")
has_mon_m_flagged.head()
```

## Aggregate to Asset Level

```{python}
#| echo: false
#| label: meters-aggregate-to-asset-level

# Aggregate to ASSETNUM level
has_mon_m_agg = has_mon_m.groupby('ASSETNUM', observed=False).agg({
    'METERNAME': 'count',  # Count of meters
    'LASTREADING_DATE': 'max',  # Most recent reading date
    'ASSET_DESC': 'first',  # Get asset description (should be same for all records)
    'CLASS': 'first',
    'DEPT': 'first'
}).reset_index()

# Rename columns for clarity
has_mon_m_agg.columns = ['ASSETNUM', 'METER_COUNT', 'MAX_LASTREADING_DATE', 'ASSET_DESC','DEPT','CLASS']

print(f"Total unique assets: {len(has_mon_m_agg):,}")
print(f"\nAggregated data sample:")
has_mon_m_agg.info()
```

## Add 1 year Threshold

```{python}
#| echo: false
#| label: meters-add-last-reading-in-a-year

# Calculate date threshold (1 year ago from today)
one_year_ago = datetime.now() - timedelta(days=365)
print(f"One year ago threshold: {one_year_ago.date()}")

# Add "Last Reading Within 1 Year" column
has_mon_m_agg['READING_WITHIN_1YR'] = has_mon_m_agg['MAX_LASTREADING_DATE'].apply(
    lambda x: 'Y' if pd.notna(x) and x >= one_year_ago else 'N'
)

# Count and display results
reading_counts = has_mon_m_agg['READING_WITHIN_1YR'].value_counts()
print(f"\nReading within 1 year distribution:")
print(reading_counts)
print(f"\nPercentage with recent readings: {reading_counts.get('Y', 0)/len(has_mon_m_agg)*100:.1f}%")

has_mon_m_agg.info()
```

## Add HAS GM column

```{python}
#| echo: false
#| label: meters-add-HAS_GM-col

# HAS_GM is only 'Y' if READING_WITHIN_1YR is 'Y'
# This follows the logic: "if this is Y... then another column (same calculation) HAS_GM = Y/N"
has_mon_m_agg['HAS_GM'] = has_mon_m_agg['READING_WITHIN_1YR'].apply(
    lambda x: 'Y' if x == 'Y' else 'N'
)

# Display results
print("Distribution of HAS_GM:")
print(has_mon_m_agg['HAS_GM'].value_counts())

# Cross-tabulation to verify logic
print("\nCross-tabulation (READING_WITHIN_1YR vs HAS_GM):")
print(pd.crosstab(has_mon_m_agg['READING_WITHIN_1YR'], has_mon_m_agg['HAS_GM']))

print("\nFinal aggregated data sample:")
has_mon_m_agg.info()
```

```{python}
#| echo: false
#| label: output-has_mon_m_agg

# Save has_mon_m_agg
has_mon_m_agg.to_pickle('data/has_mon_m_agg.pkl')
has_mon_m_agg.to_csv('output/intermediate/has_mon_m_agg.csv', index=False)

print(f"\nSaved final coverage report:")
print(f"  - data/has_mon_m_agg.pkl ({len(has_mon_m_agg):,} assets)")
print(f"  - output/intermediate/has_mon_m_agg.csv")
```

## General Meters Summary Stats

```{python}
#| echo: false
#| label: meters-summary-stats

print("=" * 60)
print("SUMMARY STATISTICS")
print("=" * 60)

print(f"\nTotal Assets: {len(has_mon_m_agg):,}")
print(f"Total Meters: {has_mon_m_agg['METER_COUNT'].sum():,}")
print(f"Average Meters per Asset: {has_mon_m_agg['METER_COUNT'].mean():.2f}")
print(f"Median Meters per Asset: {has_mon_m_agg['METER_COUNT'].median():.0f}")
print(f"Max Meters on Single Asset: {has_mon_m_agg['METER_COUNT'].max()}")

print(f"\nAssets with Recent Readings (within 1 year): {(has_mon_m_agg['READING_WITHIN_1YR'] == 'Y').sum():,}")
print(f"Assets with HAS_GM = Y: {(has_mon_m_agg['HAS_GM'] == 'Y').sum():,}")
print(f"Assets with no recent readings: {(has_mon_m_agg['READING_WITHIN_1YR'] == 'N').sum():,}")

print(f"\nAssets with null MAX_LASTREADING_DATE: {has_mon_m_agg['MAX_LASTREADING_DATE'].isna().sum():,}")
```

# PHASE IV -  HAS Merging

## Combine Route and Meter Coverage
```{python}
# Drop asset attributes from both coverage tables (we'll get from asset_class)
asset_route_coverage_slim = asset_route_coverage.drop(['ASSET_DESC', 'CLASS', 'DEPT'], axis=1)
has_mon_m_agg_slim = has_mon_m_agg.drop(['ASSET_DESC', 'CLASS', 'DEPT'], axis=1)

# Merge route and meter coverage (both are just ASSETNUM + flags now)
has_monitoring_slim = asset_route_coverage_slim.merge(
    has_mon_m_agg_slim[['ASSETNUM', 'HAS_GM', 'METER_COUNT', 'MAX_LASTREADING_DATE']],
    on='ASSETNUM',
    how='outer'
)

# FIX: Start with ALL assets from asset_class, then add monitoring data
has_monitoring = asset_class.merge(
    has_monitoring_slim,
    on='ASSETNUM',
    how='left'  # Keep ALL assets, even those with no monitoring
)

# Fill NaN values with 'N' for technologies not present
has_cols = [col for col in has_monitoring.columns if col.startswith('HAS_')]
for col in has_cols:
    has_monitoring[col] = has_monitoring[col].fillna('N')

print(f"Total assets: {len(has_monitoring):,}")
print(f"Assets with monitoring: {has_monitoring_slim['ASSETNUM'].nunique():,}")
print(f"Assets without monitoring: {len(has_monitoring) - has_monitoring_slim['ASSETNUM'].nunique():,}")

has_monitoring.info()
```

```{python}
#| echo: false
#| label: output-has_monitoring

# Save has_monitoring
has_monitoring.to_pickle('data/has_monitoring.pkl')
has_monitoring.to_csv('output/intermediate/has_monitoring.csv', index=False)

print(f"\nSaved has_monitoring:")
print(f"  - data/has_monitoring.pkl ({len(has_monitoring):,} assets)")
print(f"  - output/intermediate/has_monitoring.csv")
```

```{python}
#| echo: true
#| label: has-monitoring-summary

# Summary statistics across all technologies
print("\nHAS Monitoring Coverage Summary:")
total = len(has_monitoring)
for tech in tech_cols:
    col = f'HAS_{tech}'
    if col in has_monitoring.columns:
        count = (has_monitoring[col] == 'Y').sum()
        print(f"{tech}: {count:,} assets ({count/total*100:.1f}% of monitored assets)")
```

# PHASE V - Coverage Report: Needs vs Has

```{python}
#| label: coverage-judgment

# After Phase IV merge creates has_monitoring dataframe

# Create judgment columns based ONLY on NEEDS vs HAS
for tech in tech_cols:
    needs_col = f'NEEDS_{tech}'
    has_col = f'HAS_{tech}'
    judge_col = f'{tech.lower()}_judge'
    
    # Default: Not applicable
    has_monitoring[judge_col] = 'N'
    
    # GREEN: NEEDS and HAS (Covered)
    has_monitoring.loc[
        (has_monitoring[needs_col] == 'Y') & (has_monitoring[has_col] == 'Y'),
        judge_col
    ] = 'G'
    
    # RED: NEEDS but no HAS (Gap - Critical!)
    has_monitoring.loc[
        (has_monitoring[needs_col] == 'Y') & (has_monitoring[has_col] == 'N'),
        judge_col
    ] = 'R'
    
    # YELLOW: HAS but no NEEDS (Over-monitored)
    # Note: Could be valid if USE=Y, but we don't factor that into judgment
    has_monitoring.loc[
        (has_monitoring[needs_col] == 'N') & (has_monitoring[has_col] == 'Y'),
        judge_col
    ] = 'Y'

# Summary
print("\n" + "="*60)
print("COVERAGE JUDGMENT SUMMARY (NEEDS vs HAS)")
print("="*60)

for tech in tech_cols:
    judge_col = f'{tech.lower()}_judge'
    print(f"\n{tech}:")
    vc = has_monitoring[judge_col].value_counts()
    
    green = vc.get('G', 0)
    red = vc.get('R', 0)
    yellow = vc.get('Y', 0)
    na = vc.get('N', 0)
    
    total_needs = green + red
    coverage_pct = (green / total_needs * 100) if total_needs > 0 else 0
    
    print(f"  GREEN (Covered):        {green:6,}")
    print(f"  RED (Gap):              {red:6,}")
    print(f"  YELLOW (Over-mon):      {yellow:6,}")
    print(f"  N (Not applicable):     {na:6,}")
    print(f"  → Critical Coverage:    {coverage_pct:.1f}%")
```

```{python}
#| label: export-coverage-report

# Build comprehensive coverage report with all key metrics
coverage_report = has_monitoring.copy()

# Select columns for export
# Core asset info
asset_info_cols = ['ASSETNUM', 'ASSET_DESC', 'ASSET_CLASS', 'ASSET_DEPT']

# NEEDS flags (what must be monitored - Primary only)
needs_cols = [col for col in coverage_report.columns if col.startswith('NEEDS_')]

# HAS flags (what is currently monitored)
has_cols = [col for col in coverage_report.columns if col.startswith('HAS_')]

# Judgment flags (gap analysis)
judge_cols = [f'{tech.lower()}_judge' for tech in tech_cols]

# USE flags (optional - for analytics)
use_cols = [col for col in coverage_report.columns if col.startswith('USE_')]

# Additional metadata (if you have these columns)
metadata_cols = []
if 'ROUTE_COUNT' in coverage_report.columns:
    metadata_cols.append('ROUTE_COUNT')
if 'METER_COUNT' in coverage_report.columns:
    metadata_cols.append('METER_COUNT')
if 'MAX_LASTREADING_DATE' in coverage_report.columns:
    metadata_cols.append('MAX_LASTREADING_DATE')

# Combine all columns
export_cols = asset_info_cols + needs_cols + has_cols + judge_cols + use_cols + metadata_cols

# Filter to only include columns that exist
export_cols = [col for col in export_cols if col in coverage_report.columns]

# Create the export dataframe
coverage_report_export = coverage_report[export_cols].copy()

# Sort by class and asset number
coverage_report = coverage_report_export.sort_values(['ASSET_CLASS', 'ASSETNUM'])

# # Save final coverage report
coverage_report.to_pickle('data/coverage_report.pkl')
coverage_report.to_csv('output/coverage_report.csv', index=False)

print(f"\nSaved final coverage report:")
print(f"  - data/coverage_report.pkl ({len(coverage_report):,} assets)")
print(f"  - output/coverage_report.csv")
print(f"✓ Exported coverage report:")

print(f"  Total assets: {len(coverage_report_export):,}")
print(f"  Columns: {len(export_cols)}")
print(f"\nColumn groups:")
print(f"  Asset info: {len(asset_info_cols)}")
print(f"  NEEDS flags: {len(needs_cols)}")
print(f"  HAS flags: {len(has_cols)}")
print(f"  Judgment flags: {len(judge_cols)}")
print(f"  USE flags: {len(use_cols)}")
print(f"  Metadata: {len(metadata_cols)}")

# Show sample
print("\nSample of coverage report:")
coverage_report.head()
```

```{python}
#| label: export-summary-reports

# 1. Gap Summary by Technology
gap_summary = []

for tech in tech_cols:
    needs_col = f'NEEDS_{tech}'
    has_col = f'HAS_{tech}'
    judge_col = f'{tech.lower()}_judge'
    
    total_assets = len(coverage_report)
    needs_count = (coverage_report[needs_col] == 'Y').sum()
    has_count = (coverage_report[has_col] == 'Y').sum()
    
    green = (coverage_report[judge_col] == 'G').sum()
    red = (coverage_report[judge_col] == 'R').sum()
    yellow = (coverage_report[judge_col] == 'Y').sum()
    na = (coverage_report[judge_col] == 'N').sum()
    
    coverage_pct = (green / needs_count * 100) if needs_count > 0 else 0
    
    gap_summary.append({
        'Technology': tech,
        'Total_Assets': total_assets,
        'Assets_Needing': needs_count,
        'Assets_With_Monitoring': has_count,
        'Covered_Green': green,
        'Gap_Red': red,
        'Over_Monitored_Yellow': yellow,
        'Not_Applicable_N': na,
        'Coverage_Percent': round(coverage_pct, 1)
    })

gap_summary_df = pd.DataFrame(gap_summary)
gap_summary_df.to_csv('output/gap_summary_by_technology.csv', index=False)

print("✓ Exported gap summary by technology")
print(gap_summary_df.to_string(index=False))

# 2. Gap Summary by Asset Class
class_gap_summary = []

for asset_class in coverage_report['ASSET_CLASS'].unique():
    if pd.isna(asset_class):
        continue
    
    class_data = coverage_report[coverage_report['ASSET_CLASS'] == asset_class]
    
    # Count gaps across all technologies
    total_gaps = 0
    total_needs = 0
    
    for tech in tech_cols:
        judge_col = f'{tech.lower()}_judge'
        total_gaps += (class_data[judge_col] == 'R').sum()
        total_needs += (class_data[judge_col].isin(['G', 'R'])).sum()
    
    class_gap_summary.append({
        'Asset_Class': asset_class,
        'Asset_Count': len(class_data),
        'Total_Technology_Needs': total_needs,
        'Total_Gaps': total_gaps,
        'Gap_Rate_Percent': round((total_gaps / total_needs * 100) if total_needs > 0 else 0, 1)
    })

class_gap_summary_df = pd.DataFrame(class_gap_summary).sort_values('Total_Gaps', ascending=False)
class_gap_summary_df.to_csv('output/gap_summary_by_class.csv', index=False)

print("\n✓ Exported gap summary by class")
print(f"Classes with highest gaps:")
print(class_gap_summary_df.head(10).to_string(index=False))

# 3. Critical Gap Assets (RED judgments only)
critical_gaps = coverage_report.copy()

# Add a column counting total RED judgments per asset
critical_gaps['total_gaps'] = 0
for tech in tech_cols:
    judge_col = f'{tech.lower()}_judge'
    critical_gaps['total_gaps'] += (critical_gaps[judge_col] == 'R').astype(int)

# Filter to only assets with gaps
critical_gaps = critical_gaps[critical_gaps['total_gaps'] > 0]

# Identify which technologies have gaps
critical_gaps['missing_technologies'] = critical_gaps.apply(
    lambda row: ', '.join([
        tech for tech in tech_cols 
        if f'{tech.lower()}_judge' in row.index and row[f'{tech.lower()}_judge'] == 'R'
    ]),
    axis=1
)

# Export critical gaps
critical_export = critical_gaps[
    asset_info_cols + ['total_gaps', 'missing_technologies'] + judge_cols
].sort_values('total_gaps', ascending=False)

critical_export.to_csv('output/critical_gaps.csv', index=False)

print(f"\n✓ Exported critical gaps report")
print(f"  Assets with gaps: {len(critical_export):,}")
print(f"\nTop 10 assets with most gaps:")
print(critical_export.head(10)[['ASSETNUM', 'ASSET_DESC', 'ASSET_CLASS', 'total_gaps', 'missing_technologies']])
```


## Connectable Databases {.smaller}

::: {.columns}
::: {.column width="60%"}

```{mermaid}
%%| label: mermaid-target-DB1
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB2,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class DB1 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:3px,color:#ffffff
```
:::


::: {.column width="40%"}
**API-Enabled Systems**

- Waites (Vibration)
- Hydac (Lubrication)
- FLUKE (IR/Other)
- ZDT (Fanuc)

:::
:::

::: {.aside}
If API not available, use Disconnected Database approach
:::

## Disconnected Databases {.smaller}

::: {.columns}
::: {.column width="60%"}
```{mermaid}
%%| label: mermaid-target-DB2
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB1,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class DB2 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:4px,color:#ffffff
```
:::

::: {.column width="40%"}
**Manual Route Coding**

- FLIR (IR)
- MOTION (Vibration)
- ONTrak (Ultrasound)
- AllTest (Motor Current)
- LubeCon (Chain Monitoring)

:::
:::

::: {.fragment}
**Solution:** Standard naming conventions from MESD-ACM-0001
:::

## Route Naming Standard {.smaller}

::: {.columns}
::: {.column width="50%"}
**Standard Convention** (from MESD-ACM-0001)

![Standard Route Coding](/img/acm003_00.png)

Format: `DEPT_TECH_VENDOR - Description`
:::

::: {.column width="50%"}
**Maximo Examples**

![Examples in Maximo](/img/acm003_01.png)

::: {.fragment}
Two underscores + space-space separator
:::
:::
:::

## SQL Routes Query {.smaller}

```{mermaid}
%%| label: has-monitoring-SQL-routes
%%| fig-width: 12
flowchart LR
    DB1[Disconnected DB]
    DB2[Connectable DB]
    NM[Native Maximo]
    SQL1[SQL Routes]
    SQL2[SQL Meters]
    PY[Python Merge]
    OUT[Monitored List]
    
    DB1 --> SQL1
    DB2 --> SQL1
    DB2 -. API .-> PY
    NM --> SQL2
    SQL1 --> PY
    SQL2 --> PY
    PY --> OUT
    
    class DB2,DB1,NM source
    class SQL1,SQL2 query
    class PY process
    class OUT output
    class SQL1 target
    
    classDef source fill:#e1f5ff
    classDef query fill:#f3e5f5
    classDef process fill:#fff4e1
    classDef output fill:#e8f5e9
    classDef target fill:#fc8a7b,stroke:#b71c1c,stroke-width:4px,color:#ffffff
```

### Pattern Matching Logic

```sql
WHERE REGEXP_LIKE(ROUTE.DESCRIPTION, '^[A-Z0-9]+_[A-Z0-9]+_[A-Z0-9]+ - .+') 
```

Matches: **THREE_PART_CODE - Some text description**


::: {.aside}
Can be used in Maximo WHERE clause for saved queries
:::

## Complete Routes Query {.smaller .scrollable}

```{python}
#| echo: false
#| output: asis
#| label: routes-query

import os

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    with open(os.path.join(data_dir, "acm_assets_routes-coverage.sql"), 'r') as f:
        print(f"```sql\n{f.read()}\n```")
except FileNotFoundError:
    print("```sql\n-- SQL file not found in presentation environment\n-- See documentation for full query\n```")
```

## Routes Query Output {.smaller}

![DBeaver SQL Routes Output](/img/acm003_08.png)

::: {.fragment}
**Next Steps:** Transform into Technology & Vendor columns
:::

## Routes Transformation {auto-animate=true}

### Raw Output → Tech-Vendor Detail

![Tech-Vendor Detail](/img/acm003_10.png)

## Routes Transformation {auto-animate=true}

### Tech-Vendor Detail → Coverage Summary

![Coverage Summary](/img/acm003_09.png)

::: {.fragment}
Pivot to show each asset's technology coverage (Y/N flags)
:::

---

# Routes Coverage Analytics {background-color="#4c1d95"}

## {background-image=".\img\acm003_11.png" background-size="contain"}

::: {.notes}
Technology Coverage by Department
:::

## {background-image=".\img\acm003_12.png" background-size="contain"}

::: {.notes}
Flip perspective: how departments are covered across technologies
:::

---

# Meter-Based Monitoring {background-color="#0c4a6e"}

## Native Maximo Approach {.smaller}

::: {.columns}
::: {.column width="50%"}
![Native Maximo Flow](/img/acm003_06.png)
:::

::: {.column width="50%"}
**Direct Meter Records**

::: {.incremental}
- Readings stored in Maximo
- Direct SQL access
- Simpler than route parsing
- Native asset linkage
:::
:::
:::

## SQL Meters Query {.smaller}

![SQL Meters Flow](/img/acm003_07.png)

**Extract:** Meter Name, Last Reading Date, Asset Number

## Complete Meters Query {.smaller .scrollable}

```{python}
#| echo: false
#| output: asis
#| label: meters-query

import os

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    with open(os.path.join(data_dir, "acm_assets_meters-coverage.sql"), 'r') as f:
        print(f"```sql\n{f.read()}\n```")
except FileNotFoundError:
    print("```sql\n-- SQL file not found in presentation environment\n-- See documentation for full query\n```")
```

## Meters Query Results {.smaller}

![Meters Coverage Raw](/img/acm003_13.png)

::: {.fragment}
**Scale:** 24,091 meters applied to 8,223 assets
:::

## Data Quality Assessment {.smaller}

### Three Key Questions

::: {.incremental}
1. Which meters are **active**?
2. How many meters per asset?
3. Does asset qualify as "Has General Metering"?
:::

## Missing Data Analysis {.smaller .scrollable}

| Field | Missing Count | Percentage |
|-------|--------------|------------|
| ASSETNUM | 0 | 0.00% |
| METERNAME | 0 | 0.00% |
| METERTYPE | 0 | 0.00% |
| AVGCALCMETHOD | 19,488 | 80.89% |
| ROLLDOWNSOURCE | 19,570 | 81.23% |
| POINTNUM | 15,171 | 62.97% |
| **LASTREADING** | **5,679** | **23.57%** |
| LASTREADING_DATE | 5,568 | 23.11% |

::: {.fragment}
**Key Finding:** ~23% of meters have no reading date
:::

::: {.notes}
Available for investigation: meters_flagged_null_readings.csv
:::

## Activity Threshold Decision {.smaller}

**Question:** How old can last reading be before asset loses coverage?

::: {.fragment}
```{mermaid}
%%| fig-width: 8
flowchart LR
    A{Within 1 Year?}
    A -->|Yes| B[Has General Meter - Y]
    A -->|No| C[None - N]
    
    style B fill:#e8f5e9
    style C fill:#ffebee
```
:::

::: {.fragment}
**Decision:** 1-year threshold for active monitoring
:::

## Asset-Level Aggregation {.smaller}

![Meter Count Aggregated to Asset](/img/acm003_14.png)

::: {.incremental}
- **Count** of meters per asset
- **Max** last reading date
- Basis for coverage flag
:::

## Coverage Classification {.smaller}

![HAS_GM Column Added](/img/acm003_15.png)

::: {.columns}
::: {.column width="50%"}
**New Columns**

- `READING_WITHIN_1YR`
- `HAS_GM` (Y/N flag)
:::

::: {.column width="50%"}
::: {.fragment}
**Logic:** If max reading within 1 year → Y, else N
:::
:::
:::

---

# Data Integration {background-color="#831843"}

## Python Merge Strategy {.smaller}

::: {.callout-important}
## Documentation In Progress
Detailed merge logic being finalized
:::

::: {.fragment}
**Approach:** Pre-aggregation merge using asset master as single source of truth
:::

## Initial Coverage Results {.smaller}

| Technology | Coverage | Percentage |
|-----------|----------|------------|
| **GM** (General Metering) | 5,005 assets | 60.9% |
| **IR** (Infrared) | 360 assets | 4.4% |
| **UL** (Ultrasound) | 207 assets | 2.5% |
| **VI** (Vibration) | 345 assets | 4.2% |
| **LU** (Lubrication) | 131 assets | 1.6% |
| **MC** (Motor Circuit) | 0 assets | 0.0% |
| **ZD** (Zero Defect) | 86 assets | 1.0% |

::: {.fragment}
**Total Asset Base:** ~8,200 assets
:::

---

# Needs Monitoring {background-color="#713f12"}

## The Complexity Challenge {.smaller}

::: {.incremental}
- Each technology monitors specific **components**
- Each technology targets specific **failure modes**
- Maximo lacks **component-level granularity**
- Need systematic mapping approach
:::

## Solution: Component Mapping {.smaller}

**Reference:** Draft 001 - ACM Technology Application (Table 2)

::: {.fragment}
**Legend:**
- **P** = Primary application
- **S** = Secondary application
:::

## Component to Technology Map {.smaller .scrollable}

```{python}
#| echo: false
#| label: component-map
#| tbl-cap: Component to Technology Map

import pandas as pd

data_dir = r"C:\Users\VF033899\AppData\Roaming\DBeaverData\workspace6\General\Scripts\maximo-sql-scripts\scripts\projects\102ki-acm-2026"

try:
    component_map = pd.read_csv(f"{data_dir}/component_map.csv")
    component_map.style.hide(axis='index')
except FileNotFoundError:
    # Create sample structure if file not found
    sample_data = {
        'Component': ['Bearings', 'Motors', 'Gearboxes', 'Hydraulics'],
        'VI': ['P', 'P', 'P', 'S'],
        'IR': ['S', 'P', 'S', 'P'],
        'UL': ['P', 'S', 'S', 'P'],
        'LU': ['P', 'S', 'P', 'S']
    }
    pd.DataFrame(sample_data).style.hide(axis='index')
```

---

# Next Steps {background-color="#1e3a8a"}

## Coverage Gap Analysis {.smaller}

::: {.incremental}
1. Map asset types to required technologies
2. Compare needs vs actual coverage
3. Identify critical monitoring gaps
4. Prioritize implementation plan
:::

## Documentation & Resources {.smaller}

::: {.columns}
::: {.column width="50%"}
**Related Resources**
- MESD-ACM-0001 Standard
- Draft 001 Tech Specification
- SQL query repository
- Python merge scripts
:::

::: {.column width="50%"}
**Key Contacts**
- Stakeholder: [TBD]
- Team Members: [TBD]
- Technical Support: [TBD]
:::
:::

---

# Questions? {background-color="#1e3a8a"}

## Thank You

**Feedback & Discussion**
